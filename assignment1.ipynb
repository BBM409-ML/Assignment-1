{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e130d918-09b0-4e6c-925f-cb7f1d6c6a73",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ASSIGNMENT 1: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d36a7c-82ab-434b-9035-519447133d49",
   "metadata": {},
   "source": [
    "Algorithm which can be used for both classification as well as regression predictive problems. K-nearest neighbors (KNN) algorithm uses ‘feature similarity’ to predict the values of new datapoints which further means that the new data point will be assigned a value based on how closely it matches the points in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a6bd9-6ec1-47b0-a163-9a1f5623c11c",
   "metadata": {},
   "source": [
    "In this assignment, it is asked to implement KNN algorithm from the scratch and to apply this algorithm for both given classification and regression problems. This report shows steps we have followed, code implementations and analysis of results for KNN algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ffecc-d542-4d63-8d51-c58deaaf390c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Common Code Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea4c68f-d29c-4180-a6a9-f7ec326d9975",
   "metadata": {},
   "source": [
    "Some functions are used in both parts, thus, we prefer to gather them below this section in order to avoid unnecessary repetition in report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6e11836-d3a3-45f6-b621-439866871ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "from math import sqrt\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7a992d1-9535-496c-9244-67e9ccf4bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts given .xls file to pandas dataframe\n",
    "def read_csv_file(filename):\n",
    "    return pandas.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a5c7f8-f494-467e-b8fc-7c23d551df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates new normalized values by maintaining data distribution and scale\n",
    "# normalization is done according to min and max values in that particular column\n",
    "def normalize_dataset(dataset, target):\n",
    "    normalized_dataset = dataset.copy()\n",
    "    for column in list(dataset.columns.values):\n",
    "        if column != target:\n",
    "            normalized_dataset[column] = (normalized_dataset[column] - normalized_dataset[column].min()) / (\n",
    "                    normalized_dataset[column].max() - normalized_dataset[column].min())\n",
    "    return normalized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aeac4e-b39e-4473-978e-025fd8dfd88e",
   "metadata": {},
   "source": [
    "For machine learning, normalization is a data preparation approach in order to make the most accurate predcition. It is aimed that by normalizing the data, new values will be created that maintain the source data's basic distribution and ratios while avoiding difficulties that may arise during modeling. Instead of absolute numbers, normalized data are represented as percentile ranks in the range between 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3e4cd22-a376-44e3-bbd0-a873fb00eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits dataframe into n folds\n",
    "def cross_validation_splitter(data, n_folds):\n",
    "    dataframe_split = list()\n",
    "    # shuffle data\n",
    "    shuffle_data = data.sample(frac=1)\n",
    "    for i in range(n_folds):\n",
    "        dataframe_split.insert(i, fold_i_of_k(shuffle_data, i + 1, n_folds))\n",
    "    return dataframe_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "887fcba9-263e-40df-b800-70dfcb40060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the ith fold\n",
    "def fold_i_of_k(dataset, i, k):\n",
    "    n = len(dataset)\n",
    "    return dataset[n * (i - 1) // k:n * i // k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133f9fc-26eb-4b89-bbfd-eea885a23c19",
   "metadata": {},
   "source": [
    "K-fold cross validation is where a given data set is split into a K number of folds where each fold is used as a testing set at some point. In our assignment, we take the scenario of 5-Fold cross validation(K=5). Here, the data set is split into 5 folds. In the first iteration, the first fold is used to test the model and the rest are used to train the model. In the second iteration, 2nd fold is used as the testing set while the rest serve as the training set. This process is repeated until each fold of the 5 folds have been used as the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42dc704f-f2c1-4ae9-9da9-ce5139697ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates euclidean distance between two rows according to formula\n",
    "def calculate_euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    # iterate over each column, calculate the distance, add it to total distance\n",
    "    for i in range(len(row1[1]) - 1):\n",
    "        distance += (row1[1][i] - row2[1][i]) ** 2\n",
    "    return sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "162a713f-fb3d-4f84-bcc5-8c41d404c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the most occurred type value in the nearest neighbors\n",
    "def predict_type(nearest_neighbors_types):\n",
    "    return max(set(nearest_neighbors_types), key=nearest_neighbors_types.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4b992-551e-423f-90f6-04149909ee20",
   "metadata": {},
   "source": [
    "### PART 1: Glass Material Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b350a-ba17-4b01-bedc-b9839411e8c9",
   "metadata": {},
   "source": [
    "In this part, nearest neighbor algorithm is used in order to classify glass types of different glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5872c4c1-1f7d-468f-9722-aa9dc13a67ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this function is the main function for classification\n",
    "def classification():\n",
    "    # declares the main variables used across the functions\n",
    "    num_neighbors = [1, 3, 5, 7, 9]\n",
    "    k_folds = 5\n",
    "    data = read_csv_file(\"glass.csv.xls\")\n",
    "    target = \"Type\"\n",
    "    normalized_data = normalize_dataset(data, target)\n",
    "\n",
    "    # invokes KNN classification function by changing data and algorithm types\n",
    "    \n",
    "    # KNN classification without feature normalization\n",
    "    for num in num_neighbors:\n",
    "        scores = knn_classification(data, k_folds, num, \"normal\", target)\n",
    "        \n",
    "    # KNN classification with feature normalization\n",
    "    for num in num_neighbors:\n",
    "        scores = knn_classification(normalized_data, k_folds, num, \"normal\", target)\n",
    "\n",
    "    # weighted KNN classification without feature normalization\n",
    "    for num in num_neighbors:\n",
    "        scores = knn_classification(data, k_folds, num, \"weighted\", target)\n",
    "        \n",
    "    # KNN classification with feature normalization\n",
    "    for num in num_neighbors:\n",
    "        scores = knn_classification(normalized_data, k_folds, num, \"weighted\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6363e673-506c-4523-883a-944c39c2fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation, type predicition and accuracy calculation are done here\n",
    "def knn_classification(dataset, n_folds, k_neighbors, algo_type, target):\n",
    "    # create 5 folds from dataset\n",
    "    folds = cross_validation_splitter(dataset, n_folds)\n",
    "    results = list()\n",
    "\n",
    "    # create test and train sets\n",
    "    for i in range(len(folds)):\n",
    "        # deepcopy data so that the original dataframe does not change\n",
    "        train_set = deepcopy(folds) \n",
    "        # choose ith fold as test set and remove it from train data\n",
    "        test_set = train_set.pop(i)\n",
    "        # original -> keeps original types for accuracy metric comparison\n",
    "        original = list()\n",
    "\n",
    "        for row in folds[i].iterrows():\n",
    "            original.append(row[1][target])\n",
    "\n",
    "        predictions = list()\n",
    "        # call the related function(weighted/non-weighted) for each test data with current train set\n",
    "        # return predicted types by the written algorithm\n",
    "        for row in test_set.iterrows():\n",
    "            if algo_type == \"normal\":\n",
    "                output = predict_type(get_nearest_neighbors(train_set, row, k_neighbors))\n",
    "                predictions.append(output)\n",
    "            elif algo_type == \"weighted\":\n",
    "                output = predict_weighted_type(train_set, row, k_neighbors)\n",
    "                predictions.append(output)\n",
    "\n",
    "        # calculate accuracy metric with original and predicted types\n",
    "        accuracy = accuracy_metric(original, predictions)\n",
    "        results.append(accuracy)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae21bf1-74cf-484d-b738-012d3b91fb7e",
   "metadata": {},
   "source": [
    "In this function, first of all we split the data into 5-folds. Then, we iterate over the folds and in each iteration, we choose ith fold as test data and the remaining ones as the train data. For each test row, we get the nearest neighbors and predict its type. Lastly, we compare the predicted and actual values and calculate the accuracy of our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4893f1f4-db8c-41d4-8ab7-cb872da9c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the nearest neighbors of the test row\n",
    "def get_nearest_neighbors(train_data, test_row, k):\n",
    "    # distances are kept as dictionary because we need both type and distance \n",
    "    # in order to sort distance and return the nearest ones' types\n",
    "    distances = dict()\n",
    "\n",
    "    for train in train_data:\n",
    "        for train_row in train.iterrows():\n",
    "            #calculate distance for each train row and test row\n",
    "            distances[calculate_euclidean_distance(train_row, test_row)] = train_row[1][\"Type\"]\n",
    "    # sort the dictionary values according to distances -> keys and get first k ones\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[0])[:k]\n",
    "    # return only the types\n",
    "    return [item[1] for item in sorted_distances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98cb7736-ab9c-4702-b17f-610242ac3ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_weighted_type(train_data, test_row, k):\n",
    "    distances = dict()\n",
    "\n",
    "    # finds nearest neighbors\n",
    "    for train in train_data:\n",
    "        for train_row in train.iterrows():\n",
    "            distances[calculate_euclidean_distance(train_row, test_row)] = train_row[1][\"Type\"]\n",
    "    \n",
    "    # get the first k neighbors\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[0])[:k]\n",
    "\n",
    "    # calculates weighted distances from neighbors\n",
    "    weight = dict()\n",
    "    for distance in sorted_distances:\n",
    "        if weight.get(distance[1]) and distance[0] != 0:\n",
    "            weight[distance[1]] = weight[distance[1]] + (1 / distance[0])\n",
    "        elif (not weight.get(distance[1])) and distance[0] != 0:\n",
    "            weight[distance[1]] = (1 / distance[0])\n",
    "        elif distance[0] == 0:\n",
    "            weight[distance[1]] = 0\n",
    "    \n",
    "    # returns the most weighted type\n",
    "    return max(weight, key=weight.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d900c-1ed9-4ac7-91d8-15b1800cf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the accuracy metric according to given formula\n",
    "def accuracy_metric(original, predicted):\n",
    "    correct = 0\n",
    "    # compares predicted and original types\n",
    "    for i in range(len(original)):\n",
    "        if original[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(original)) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c5385-77fb-4ec4-89ab-ba592402245f",
   "metadata": {},
   "source": [
    "### PART 2: Concrete Material Strength Estimation from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8be6fd07-7738-47f1-9085-abc6f1c97e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is the main function for regression\n",
    "def regression():\n",
    "    num_neighbors = [1, 3, 5, 7, 9]\n",
    "    k_folds = 5\n",
    "    data = read_csv_file(\"Concrete_Data_Yeh.csv\")\n",
    "    target = \"csMPa\"\n",
    "    normalized_data = normalize_dataset(data, target)\n",
    "    \n",
    "    # invokes KNN regression function by changing data and algorithm types\n",
    "\n",
    "     # KNN regression without feature normalization\n",
    "    for num in num_neighbors:\n",
    "        scores = knn_regression(data, k_folds, num, \"normal\", target)\n",
    "   \n",
    "     # KNN regression with feature normalization\n",
    "    for num in num_neighbors:\n",
    "        scores = knn_regression(normalized_data, k_folds, num, \"normal\", target)\n",
    "        \n",
    "    # weighted KNN regression without feature normalization\n",
    "    for num in num_neighbors:\n",
    "        scores = knn_regression(data, k_folds, num, \"weighted\", target)\n",
    "\n",
    "    # weighted KNN regression with feature normalization\n",
    "    for num in num_neighbors:\n",
    "        scores = knn_regression(normalized_data, k_folds, num, \"weighted\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fd82727-3ded-4825-844e-c5ca3bf91dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation, csMPa prediction and mean absolute error calculation are done here\n",
    "def knn_regression(dataset, n_folds, k_neighbors, algo_type, target):\n",
    "    # splits data into 5 folds\n",
    "    folds = cross_validation_splitter(dataset, n_folds)\n",
    "    results = list()\n",
    "\n",
    "    # create test and train sets\n",
    "    for i in range(len(folds)):\n",
    "        train_set = deepcopy(folds)\n",
    "        test_set = train_set.pop(i)\n",
    "        # original -> keeps original types for accuracy metric comparison\n",
    "        original = list()\n",
    "\n",
    "        for row in folds[i].iterrows():\n",
    "            original.append(row[1][target])\n",
    "\n",
    "        # according to algorithm type, call prediction functions\n",
    "        predictions = list()\n",
    "        for row in test_set.iterrows():\n",
    "            if algo_type == \"normal\":\n",
    "                output = predict_csMPa(train_set, row, k_neighbors, target)\n",
    "                predictions.append(output)\n",
    "            elif algo_type == \"weighted\":\n",
    "                output = predict_weighted_csMPa(train_set, row, k_neighbors, target)\n",
    "                predictions.append(output)\n",
    "        \n",
    "        # calculate mean absolute error\n",
    "        mea = mean_absolute_error(original, predictions)\n",
    "        results.append(mea)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81a1ed4f-b0da-441b-be76-8adfdf99a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_csMPa(train_data, test_row, k, target):\n",
    "    distances = dict()\n",
    "\n",
    "    for train in train_data:\n",
    "        for train_row in train.iterrows():\n",
    "            distances[calculate_euclidean_distance(train_row, test_row)] = train_row[1][target]\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[0])[:k]\n",
    "    # get the average of distances\n",
    "    return reduce(lambda a, b: a + b, [item[1] for item in sorted_distances]) / len([item[1] for item in sorted_distances])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6674aeb8-9ca2-4bdf-87e7-4eead8e2ddd8",
   "metadata": {},
   "source": [
    "We are looping over each point of the test data set, to find the euclidian distance between the test point and train data points. Then we are sorting the distance and finding the nearest K neighbours. Since we have the target values of the nearest neighbours, we average of those values and equate the predicted target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d2b7ed8-48fc-4681-a597-203b137f3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_weighted_csMPa(train_data, test_row, k, target):\n",
    "    distances = dict()\n",
    "\n",
    "    for train in train_data:\n",
    "        for train_row in train.iterrows():\n",
    "            distances[calculate_euclidean_distance(train_row, test_row)] = train_row[1][target]\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[0])[:k]\n",
    "    \n",
    "    # x -> calculates weighted distance, y -> calculates total distance \n",
    "    x = reduce(lambda a, b: a + b, [(1/item[0]*item[1]) if item[0] != 0 else (item[1]) for item in sorted_distances])\n",
    "    y = reduce(lambda a, b: a + b, [(1/item[0]) if item[0] != 0 else (item[1]) for item in sorted_distances])\n",
    "\n",
    "    return x/y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb14e9-6b18-4cc6-90c8-efdce6cf7555",
   "metadata": {},
   "source": [
    "In weighted k-NN model, the algorithm will still look at all k nearest neighbors, but the closer neighbors will have more of a vote than those further away.  The \"predict_weighted_csMPa\" function predicts the target variable \"csMPa\" of the query point using distance-weighted voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b86a2e23-ef92-4463-9fa8-3490da20289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(original, predicted):\n",
    "    error = 0\n",
    "    for i in range(len(original)):\n",
    "        error += abs(original[i] - predicted[i])\n",
    "    return error / len(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706c5b9-4afb-40ae-8b90-5f55fbf3493e",
   "metadata": {},
   "source": [
    "The \"mean_absolute_error\" function allows us to measure the accuracy of the model by comparing the actual values ​​and the predicted values. The returned value tells us that the average difference between the actual data value and the value predicted by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8154f3-7cd8-4ab7-bf65-cff77168aa76",
   "metadata": {},
   "source": [
    "### ANALYSIS FOR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac8a85-9ff4-4ac5-8574-106bc3c1793e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfbe41-0776-480c-be9c-d561e9713d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
